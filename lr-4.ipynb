{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe Model with Learning Rate = 1e-4\n",
    "\n",
    "**Layer Sizes**\n",
    "- H1 = 64\n",
    "\n",
    "- Glove 50d\n",
    "- Optimizer : NAdam\n",
    "- Batch Size = 64\n",
    "- Epochs : 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: adabound in /usr/local/lib/python3.9/site-packages (0.0.5)\n",
      "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.9/site-packages (from adabound) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from torch>=0.4.0->adabound) (4.0.1)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v9/0g473_2s1_7d8mjnpg8tx86r0000gn/T/ipykernel_1524/246259765.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;31m###########################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/nltk/chunk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    153\u001b[0m \"\"\"\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChunkParserI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpChunkParser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRegexpParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m from nltk.chunk.util import (\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/nltk/chunk/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChunkScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParserI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/nltk/parse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mProjectiveDependencyParser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m )\n\u001b[0;32m---> 95\u001b[0;31m from nltk.parse.recursivedescent import (\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mRecursiveDescentParser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mSteppingRecursiveDescentParser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#download glove dataset\n",
    "!test -f glove.6B.50d.txt ||  ( wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip && unzip glove.6B.zip)\n",
    "#download adabound\n",
    "!pip3 install adabound\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk \n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from sklearn.feature_extraction.text import  HashingVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from numpy.ma.core import concatenate\n",
    "from itertools import cycle\n",
    "from adabound import AdaBound\n",
    "\n",
    "\n",
    "\n",
    "glove = pd.read_csv('glove.6b.50d.txt', sep=' ', index_col=0, header=None, quoting=3)\n",
    "glove_embedding = {key: val.values for key, val in glove.T.items()}\n",
    "\n",
    "\n",
    "\n",
    "data_train = pd.read_csv('./vaccine_train_set.csv')\n",
    "data_val = pd.read_csv('./vaccine_validation_set.csv')\n",
    "\n",
    "x_train = pd.DataFrame(data_train,columns=[data_train.columns[1]])\n",
    "y_train = pd.DataFrame(data_train,columns=[data_train.columns[2]])\n",
    "\n",
    "x_val =  pd.DataFrame(data_val,columns=[data_val.columns[1]])\n",
    "y_val =  pd.DataFrame(data_val,columns=[data_val.columns[2]])\n",
    "\n",
    "def create_array(tweet): #split tweet into array of its words\n",
    "    return re.split(' ',tweet)\n",
    "\n",
    "def transform(tweet):\n",
    "    array = np.array(np.zeros([1,50]))\n",
    "    for word in tweet:\n",
    "        if word not in glove_embedding: #glove does not contain the word in the tweet, fill features with 0\n",
    "            ar = np.zeros([1,50])\n",
    "        else:\n",
    "            ar = np.array([glove_embedding.get(word)]) #fill row features with glove's word embedding\n",
    "        array = np.concatenate((array,ar)) #concatenate the word embedding with the tweet embedding\n",
    "    return array\n",
    "\n",
    "def sum_array(tweet): #sum the array of word embeddings to get the tweet embedding\n",
    "    return np.sum(tweet, axis=0) \n",
    "\n",
    "\n",
    "\n",
    "#create,transform,sum the tweet embedding\n",
    "#train set\n",
    "x_train['tweet'] = x_train['tweet'].apply(lambda tweet:create_array(tweet))\n",
    "x_train['tweet'] = x_train['tweet'].apply(lambda tweet:transform(tweet))\n",
    "x_train['tweet'] = x_train['tweet'].apply(lambda tweet:sum_array(tweet))\n",
    "\n",
    "#validation set\n",
    "x_val['tweet'] = x_val['tweet'].apply(lambda tweet:create_array(tweet))\n",
    "x_val['tweet'] = x_val['tweet'].apply(lambda tweet:transform(tweet))\n",
    "x_val['tweet'] = x_val['tweet'].apply(lambda tweet:sum_array(tweet))\n",
    "\n",
    "\n",
    "\n",
    "#create the train tensor \n",
    "x_train = torch.tensor(x_train['tweet'],dtype=torch.float)\n",
    "k_train = np.array(y_train['label'])\n",
    "\n",
    "y_train = torch.from_numpy(k_train)\n",
    "# y_train = y_train.to(torch.long)\n",
    "\n",
    "#create the validation tensor \n",
    "x_val = torch.tensor(x_val['tweet'],dtype=torch.float)\n",
    "k_val = np.array(y_val['label'])\n",
    "\n",
    "y_val = torch.from_numpy(k_val)\n",
    "\n",
    "\n",
    "\n",
    "#Define the NN\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H1, D_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, D_out)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.linear1(x)\n",
    "        act1 =  nn.functional.relu(h1)\n",
    "\n",
    "        out = self.linear2(act1)\n",
    "        return out\n",
    "\n",
    "#Define layer sizes\n",
    "D_in = x_train.shape[1]\n",
    "H1 = 64\n",
    "D_out = 3\n",
    "\n",
    "#Define Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "\n",
    "#Initialize model, loss, optimizer\n",
    "model = Net(D_in, H1, D_out)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#Initialize train dataloader\n",
    "batch_size = 64\n",
    "dataset_train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "#Initialize validation dataloader\n",
    "dataset_val = torch.utils.data.TensorDataset(x_val,y_val)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size, shuffle=True)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "iterations,losses_train,losses_val,f1ScoresTrain,f1ScoresVal,recallTrain,recallVal,precisionTrain,precisionVal = [],[],[],[],[],[],[],[],[]\n",
    "n = 0\n",
    "for epoch in range(50):\n",
    "  batch_losses_train = []\n",
    "  batch_losses_val = []\n",
    "  f1_batch_scores_train,f1_batch_scores_val = [],[]\n",
    "  precision_batch_train,precision_batch_val = [],[]\n",
    "  recall_batch_train,recall_batch_val = [],[]\n",
    "\n",
    "  for x_batch, y_batch in dataloader_train:\n",
    "    y_pred_train = model(x_batch)\n",
    "    loss = loss_func(y_pred_train, y_batch)\n",
    "    batch_losses_train.append(loss.item())\n",
    "\n",
    "    #Delete previously stored gradients\n",
    "    optimizer.zero_grad()\n",
    "    #Perform backpropagation starting from the loss calculated in this epoch\n",
    "    loss.backward()\n",
    "    #Update model's weights based on the gradients calculated during backprop\n",
    "    optimizer.step()\n",
    "    n += 1\n",
    "\n",
    "    model.eval()\n",
    "    model_batch_result = nn.functional.softmax(y_pred_train,dim=1)\n",
    "    y_pred = model_batch_result.cpu().max(1)[1].numpy()\n",
    "    f1Score = metrics.f1_score(y_batch,y_pred,average='weighted')\n",
    "    precision = metrics.precision_score(y_batch,y_pred,average='weighted',zero_division=0)\n",
    "    recall = metrics.recall_score(y_batch,y_pred,average='weighted',zero_division=0)\n",
    "    f1_batch_scores_train.append(f1Score)\n",
    "    precision_batch_train.append(precision)\n",
    "    recall_batch_train.append(recall)\n",
    "  \n",
    "  print(f'\\nEpoch: {epoch:3}')\n",
    "  print(f\"(Training) - Loss = {sum(batch_losses_train)/len(dataloader_train):.5f}\")\n",
    "  print(f\"(Training) - F1 Score = {sum(f1_batch_scores_train)/len(dataloader_train):.5f}\")\n",
    "\n",
    "\n",
    "  losses_train.append(sum(batch_losses_train)/len(dataloader_train))\n",
    "  f1ScoresTrain.append(sum(f1_batch_scores_train)/len(dataloader_train))\n",
    "  precisionTrain.append(sum(precision_batch_train)/len(dataloader_train))\n",
    "  recallTrain.append(sum(recall_batch_train)/len(dataloader_train))\n",
    "    \n",
    "\n",
    "  #evaluate the model on the validation set\n",
    "  for x_batch,y_batch in dataloader_val:\n",
    "    y_pred_val = model(x_batch)\n",
    "    loss_val = loss_func(y_pred_val,y_batch)\n",
    "    batch_losses_val.append(loss_val.item())\n",
    "\n",
    "    model.eval()\n",
    "    model_batch_result = nn.functional.softmax(y_pred_val,dim=1)\n",
    "    y_pred = model_batch_result.cpu().max(1)[1].numpy()\n",
    "    f1Score = metrics.f1_score(y_batch,y_pred,average='weighted' )\n",
    "    precision = metrics.precision_score(y_batch,y_pred,average='weighted',zero_division=0)\n",
    "    recall = metrics.recall_score(y_batch,y_pred,average='weighted',zero_division=0)\n",
    "    f1_batch_scores_val.append(f1Score)\n",
    "    precision_batch_val.append(precision)\n",
    "    recall_batch_val.append(recall)\n",
    "\n",
    "  losses_val.append(sum(batch_losses_val)/len(dataloader_val))\n",
    "  f1ScoresVal.append(sum(f1_batch_scores_val)/len(dataloader_val))\n",
    "  precisionVal.append(sum(precision_batch_val)/len(dataloader_val))\n",
    "  recallVal.append(sum(recall_batch_val)/len(dataloader_val))\n",
    "\n",
    "  print(f\"(Validation) - Loss = {sum(batch_losses_val)/len(dataloader_val):.5f}\")\n",
    "  print(f\"(Validation) - F1 Score = {sum(f1_batch_scores_val)/len(dataloader_val):.5f}\")\n",
    "\n",
    "  iterations.append(n)\n",
    "  # scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "# Plotting\n",
    "\n",
    "#Loss\n",
    "plt.title(\"Training Curve (batch_size={}, lr={})\".format(batch_size, learning_rate))\n",
    "plt.plot(iterations, losses_train, label=\"Train\")\n",
    "plt.plot(iterations, losses_val, label=\"Validation\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "#F1 Score\n",
    "plt.title(\"F1 Score (batch_size={}, lr={})\".format(batch_size, learning_rate))\n",
    "plt.plot(iterations, f1ScoresTrain, label=\"Train\")\n",
    "plt.plot(iterations, f1ScoresVal, label=\"Validation\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "#Precision\n",
    "plt.title(\"Precision (batch_size={}, lr={})\".format(batch_size, learning_rate))\n",
    "plt.plot(iterations, precisionTrain, label=\"Train\")\n",
    "plt.plot(iterations, precisionVal, label=\"Validation\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "#recall\n",
    "plt.title(\"Recall (batch_size={}, lr={})\".format(batch_size, learning_rate))\n",
    "plt.plot(iterations, recallTrain, label=\"Train\")\n",
    "plt.plot(iterations, recallVal, label=\"Validation\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#roc curve\n",
    "y_val_roc = label_binarize(y_val, classes=[0, 1, 2])\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "y_val_roc = torch.tensor(y_val_roc)\n",
    "\n",
    "y_pred_val = model(x_val)\n",
    "sc = nn.functional.softmax(y_pred_val,dim=1)\n",
    "\n",
    "for i in range(3):\n",
    "  fpr[i], tpr[i], _ = roc_curve(y_val_roc.detach().numpy()[:, i], sc.detach().numpy()[:, i])\n",
    "  roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "fpr[3], tpr[3], _ = roc_curve(y_val_roc.detach().numpy().ravel(), sc.detach().numpy().ravel())\n",
    "roc_auc[3] = auc(fpr[3], tpr[3])\n",
    "\n",
    "colors = cycle(['blue', 'red', 'green','orange'])\n",
    "for i, color in zip(range(4), colors):\n",
    "  if(i == 3):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve (area = {0:0.2f})'\n",
    "             ''.format(roc_auc[i]))\n",
    "  else:\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1.5)\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for multi-class data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
